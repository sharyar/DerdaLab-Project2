{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create PairWise Training and Test Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Create a common data frame with 8000*2, 1 label, and n rows of samples. Call this **mainDF**\n",
    "2. Use **sourceDF** to do a stratified random sampling of the samples so we have even distribution of labels. \n",
    "3. Append 2 samples to mainDF and add label based on the samples added\n",
    "4. Labels: Not siblings, siblings\n",
    "5. The label is based on filename again where two samples are siblings if: XXXXX-RPn everything except n matches. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "vscode": {
     "end_execution_time": "2020-08-05T22:47:08.583Z",
     "start_execution_time": "2020-08-05T22:47:08.045Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from fileFunctionsModule import importCSVMotifFilesAsDfFromDir\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import time\n",
    "from datetime import date\n",
    "import sys\n",
    "import os.path\n",
    "import re\n",
    "from itertools import permutations\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Import & Early Processing to a Single DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "vscode": {
     "end_execution_time": "2020-08-05T22:47:11.910Z",
     "start_execution_time": "2020-08-05T22:47:08.584Z"
    }
   },
   "outputs": [],
   "source": [
    "# ! Define variables to be used\n",
    "rerun_data_compilation = False\n",
    "data_path = \"\"\n",
    "data_type = \"\"\n",
    "date_current = str(date.today().isoformat())\n",
    "\n",
    "# Ask user what kind of data needs to be processed and if data should be recompiled from source csv files. \n",
    "while (data_type.lower() not in ['trimer', 'tetramer']) :\n",
    "    data_type = input('Process Trimer or Tetramer data type?')\n",
    "    if (input('Would you like to rerun data compilation? Y or N').upper() == 'N'):\n",
    "        rerun_data_compilation = False\n",
    "    else:\n",
    "        rerun_data_compilation = True\n",
    "\n",
    "if data_type == 'trimer':\n",
    "    data_path = \"../03_SourceFiles/01_TrimerSourceFiles\"\n",
    "elif data_type == 'tetramer':\n",
    "    data_path = \"../03_SourceFiles/02_TetramerSourceFiles\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": [],
    "vscode": {
     "end_execution_time": "2020-08-05T22:47:13.398Z",
     "start_execution_time": "2020-08-05T22:47:11.916Z"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "File read succsessful. Data frame loaded into memory.\n"
    }
   ],
   "source": [
    "# Checks if user wants to rerun data compilation. If not, old label file is used from memory\n",
    "if rerun_data_compilation:\n",
    "    # Import Data\n",
    "    # transpose axis of the dataframe:\n",
    "    # Fill na and nan values with 0 instead\n",
    "    data_start = importCSVMotifFilesAsDfFromDir(data_path).transpose().reset_index().fillna(0)\n",
    "\n",
    "    # Rename index column to sampleName in place\n",
    "    data_start.rename(columns={'index': 'sampleName'}, inplace=True)\n",
    "\n",
    "    # Add a new column with a binary identifier for a naive vs selected library. i.e = 1 is naive and 0 is selected\n",
    "    data_start['naiveLibrary'] = data_start.apply(\n",
    "        lambda row: (('oo' in row.sampleName[10:21]) and\n",
    "                    ('RN0' in row.sampleName[-6:] or 'RN1' in row.sampleName[-6:])),\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "    # convert the data in the column (NaiveLibrary) to int type so it is readable\n",
    "    data_start['naiveLibrary'] = data_start['naiveLibrary'].astype(int)\n",
    "\n",
    "    if (input('Would you like to save the resulting DF as a csv for future use or overwrite the previously saved csv? Y or N').upper() == 'Y'):\n",
    "        data_start.to_csv(str('../03_SourceFiles/03_ProcessedFiles/'+data_type+'-combined-labeled.csv'))\n",
    "        print('DF Structure Saved to file.')\n",
    "    else:\n",
    "        pass\n",
    "elif not rerun_data_compilation:\n",
    "    if os.path.isfile(str('../03_SourceFiles/03_ProcessedFiles/'+data_type+'-combined-labeled.csv')):\n",
    "        try:\n",
    "            data_start = pd.read_csv(\n",
    "                str('../03_SourceFiles/03_ProcessedFiles/'+data_type+'-combined-labeled.csv'),\n",
    "                engine='c',\n",
    "                low_memory=False,\n",
    "                index_col=0\n",
    "                )\n",
    "            print('File read succsessful. Data frame loaded into memory.')\n",
    "        except:\n",
    "            print('Something went wrong while trying to read the csv file. Please rerun data compilation or check file manually.')\n",
    "            sys.exit()\n",
    "    else:\n",
    "        print('CSV File does not exist in the source folder (03_SourceFiles/03_ProcessedFiles). Please re-run compilation')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find # of siblings in data set & separate them out into sibling groups. \n",
    "\n",
    "1. Create a naive and a selected set. \n",
    "2. Create two dictionaries: one containing groups of naive siblings and one for selected siblings. \n",
    "3. Enumerate sets to use for stats later. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "vscode": {
     "end_execution_time": "2020-08-05T22:47:13.440Z",
     "start_execution_time": "2020-08-05T22:47:13.399Z"
    }
   },
   "outputs": [],
   "source": [
    "naive_sampleName_set = data_start[(data_start['naiveLibrary'] == 1)]['sampleName']\n",
    "selected_sampleName_set = data_start[~(data_start['naiveLibrary'] == 1)]['sampleName']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logic of setting up groups:\n",
    "\n",
    "1. Set key of dictionary to 20170808-109OOooNA-JB-3_RN**X** where X indicates round. The Primer and Replicate are ommitted\n",
    "2. Append value of sampleName to list within each diction by matching the key. Use Regular Expressions to create and match key. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "vscode": {
     "end_execution_time": "2020-08-05T22:47:13.447Z",
     "start_execution_time": "2020-08-05T22:47:13.440Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define required functions\n",
    "\n",
    "def find_sibling_groups(data_iterable):\n",
    "    \"\"\"\n",
    "    Creates a dictionary from provided iterable. The iterable should be made up of sampleNames in this format: \n",
    "    20170808-109OOooNA-JB-3__R10F5_RN1RP3\n",
    "    The keys of the dictionary are set to: 20170808-109OOooNA-JB-3_RN1 (The primer and replicate info is removed)\n",
    "    \"\"\"\n",
    "    sibling_group_dict = {}\n",
    "\n",
    "    for index, value in data_iterable.items():\n",
    "        keyID = value.split('__')[0] + '_' + re.findall(r'RN\\d{1,}' , value)[0]\n",
    "        if keyID not in sibling_group_dict:\n",
    "            sibling_group_dict[keyID] = [value]\n",
    "        else:\n",
    "            if value not in sibling_group_dict[keyID]:\n",
    "                sibling_group_dict[keyID].append(value)\n",
    "    print('Sibbling Groups Founds: ', len(sibling_group_dict))\n",
    "    return sibling_group_dict\n",
    "\n",
    "\n",
    "def remove_single_member_sibling_groups(sibling_group_original, return_extra=False):\n",
    "    \"\"\"\n",
    "    Iterates through the provided dictionary and finds any sibblings groups with one member and removes them.\n",
    "    Returns three values: \n",
    "        sibbling group as a dict with single groups removed\n",
    "        single_group_value -> values within the single groups\n",
    "        empty_group -> returns any keys in dictionary which had empty lists (unlikely to happen)\n",
    "\n",
    "    \"\"\"\n",
    "    sibling_group = sibling_group_original.copy()\n",
    "    single_group_key = []\n",
    "    single_group_value = []\n",
    "    empty_group = []\n",
    "    for key, value in sibling_group.items():\n",
    "        if len(value) == 1:\n",
    "            single_group_value.append(value)\n",
    "            single_group_key.append(key)\n",
    "        elif len(value) < 1:\n",
    "            empty_group.append(key)\n",
    "    \n",
    "    for key_ in single_group_key:\n",
    "        sibling_group.pop(key_)\n",
    "    for key_ in empty_group:\n",
    "        sibling_group.pop(key_)\n",
    "    if len(single_group_key) > 0:\n",
    "        print('Single Groups Found: ', len(single_group_value), '\\nSibling Groups: ', len(sibling_group))\n",
    "    if len(empty_group) > 0:\n",
    "        print(\"Empty Groups Found\")\n",
    "\n",
    "    if return_extra:\n",
    "        return sibling_group, single_group_value, empty_group\n",
    "    else:\n",
    "        return sibling_group\n",
    "\n",
    "def remove_mismatching_sibling_groups(sibling_group_original, return_extra=False):\n",
    "    \"\"\"\n",
    "    Iterates through provided sibbling dictionary to find any mismatches beteween key name and values within the list associated with the key. \n",
    "    Return_extra is set to false by default so only the sibbling_group dictionary is return. Can be changed to return:\n",
    "        sibbling_group -> Dictionary\n",
    "        mismatching_groups -> Dictionary (includes the entire group that had a mismatch)\n",
    "        mismatching_groups -> Values that were mismatched (without their keys)\n",
    "    \"\"\"\n",
    "\n",
    "    sibling_group = sibling_group_original.copy()\n",
    "    mismatching_groups ={}\n",
    "    mismatching_group_values = []\n",
    "    for key, value in sibling_group.items():\n",
    "        group_id = re.findall(r'RN\\d{1,}', key)[0]\n",
    "        for sample in value:\n",
    "            if group_id != re.findall(r'RN\\d{1,}', sample)[0]:\n",
    "                mismatching_groups[key] = value\n",
    "                mismatching_group_values.append(sample)\n",
    "\n",
    "    for key in mismatching_groups:\n",
    "        sibling_group.pop(key)\n",
    "\n",
    "    if len(mismatching_groups) > 0:\n",
    "        print('Mismatches Found, rerun with return_extra=True as an argument to get mismatching values')\n",
    "    \n",
    "    if (return_extra):\n",
    "        return sibling_group, mismatching_groups, mismatching_group_values\n",
    "    else:\n",
    "        return sibling_group\n",
    "\n",
    "def find_average_sibling_members(sibling_group):\n",
    "    total_members = 0\n",
    "    for key, value in sibling_group.items():\n",
    "        total_members += len(value)\n",
    "    return total_members/(len(sibling_group))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Sibbling Groups Founds:  255\nSingle Groups Found:  53 \nSibling Groups:  202\nSibbling Groups Founds:  448\nSingle Groups Found:  73 \nSibling Groups:  375\n"
    }
   ],
   "source": [
    "#### Create Sibbling Groups:\n",
    "naive_sibling_group_dict = find_sibling_groups(naive_sampleName_set)\n",
    "naive_sibling_group_dict = remove_mismatching_sibling_groups(remove_single_member_sibling_groups(naive_sibling_group_dict))\n",
    "\n",
    "selected_sibling_group_dict = find_sibling_groups(selected_sampleName_set)\n",
    "selected_sibling_group_dict = remove_mismatching_sibling_groups(remove_single_member_sibling_groups(selected_sibling_group_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Combine the two sibling dictionaries into one:\n",
    "\n",
    "combined_sibling_group_dict = {**naive_sibling_group_dict, **selected_sibling_group_dict}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extra for stats only\n",
    "# labels_bar_siblings = ['Naive SG', 'Selected SG', 'Naive Singles', 'Selected Singles']\n",
    "# data_bar_siblings = [\n",
    "#     len(naive_sibling_groups), \n",
    "#     len(selected_sibling_groups),\n",
    "#     len(n_s_g),\n",
    "#     len(s_s_g)\n",
    "#     ]\n",
    "# plt.bar(labels_bar_siblings, data_bar_siblings)\n",
    "# plt.title('Number of Sibling Groups')\n",
    "# plt.xlabel('Singles Indicate Sibling Groups with 1 Member',fontdict={\n",
    "#     'weight':'normal',\n",
    "#     'size':'13'\n",
    "# },  labelpad=10)\n",
    "\n",
    "# plt.savefig(str(data_type+'-sibling-groups-distribution-'+ date_current + '.png'), bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "3.7475247524752477\n4.037333333333334\n"
    }
   ],
   "source": [
    "print(find_average_sibling_members(naive_sibling_group_dict))\n",
    "print(find_average_sibling_members(selected_sibling_group_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_sibling_list(sibling_dict):\n",
    "    list_left = []\n",
    "    list_right = []\n",
    "    for key,value in sibling_dict.items():\n",
    "        for a,b in permutations(value, 2):\n",
    "            list_left.append(a)\n",
    "            list_right.append(b)\n",
    "        \n",
    "    return list_left, list_right\n",
    "\n",
    "def generate_df_from_permutations(df, sibling_dict, sibling_label):\n",
    "    if sibling_label:\n",
    "        list_left, list_right = create_sibling_list(sibling_dict)\n",
    "    else:\n",
    "        list_left, list_right = zip(*sibling_dict)\n",
    "    df_left = pd.DataFrame(list_left, columns=['sampleName'], index=None)\n",
    "    df_left = pd.merge(df_left, df, how='left', on='sampleName', sort=False, validate='m:1')\n",
    "    df_right = pd.DataFrame(list_right, columns=['sampleName'], index=None)\n",
    "    df_right = pd.merge(df_right, df, how='left', on='sampleName', sort=False, validate='m:1')\n",
    "    df_left = df_left.add_prefix('l_')\n",
    "    df_right = df_right.add_prefix('r_')\n",
    "    df_combined = pd.concat([df_left, df_right], axis=1)\n",
    "    df_combined['sibling'] = sibling_label\n",
    "    df_combined['c_sampleName'] = df_combined['l_sampleName'] + '_^_' + df_combined['r_sampleName']\n",
    "    print('DF Created Successfully. Df shape: \\n', df_combined.shape)\n",
    "    return df_combined\n",
    "\n",
    "def generate_random_non_sibling_df(df, sibling_dict, sample_size=10000, r_seed=42):\n",
    "    # converts the lists of lists within the sibling_dict to a flat list\n",
    "    flat_list = set([item for sublist in sibling_dict.values() for item in sublist])\n",
    "    all_permutations = set(permutations(flat_list, 2))\n",
    "    print('All Possible Permutations:', len(list(all_permutations)))\n",
    "    \n",
    "    list_left, list_right = create_sibling_list(sibling_dict)\n",
    "    permutations_siblings = set(tuple(zip(list_left, list_right)))\n",
    "    print('Sibling Permutations: ', len(permutations_siblings))\n",
    "\n",
    "    permutation_non_sibling_dict = all_permutations.difference(permutations_siblings)\n",
    "    print('Non-Sibling Permutations: ', len(permutation_non_sibling_dict))\n",
    "\n",
    "    random.seed(r_seed)\n",
    "    print('Sampling Seed Used: ', r_seed)\n",
    "    permutation_non_sibling_sampled_dict = random.sample(permutation_non_sibling_dict, sample_size)\n",
    "\n",
    "    return generate_df_from_permutations(df, permutation_non_sibling_sampled_dict, 0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "DF Created Successfully. Df shape: \n (10588, 16006)\n"
    }
   ],
   "source": [
    "sibling_df = generate_df_from_permutations(data_start, combined_sibling_group_dict, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "All Possible Permutations: 5155170\nSibling Permutations:  10588\nNon-Sibling Permutations:  5144582\nSampling Seed Used:  42\nDF Created Successfully. Df shape: \n (12000, 16006)\n"
    }
   ],
   "source": [
    "non_sibling_df = generate_random_non_sibling_df(data_start, combined_sibling_group_dict, 12000, 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "                                l_sampleName  \\\n0        20171128-71NYsaVH-VG-3__R1F2_RN2RP2   \n1        20161105-13OOicXZ-JW-3__R3F2_RN2RP1   \n2        20170228-22OOooNA-HD-3__R2F3_RN1RP4   \n3       20150819-07OObwUD-OO-3__R5F18_RN2RP2   \n4        20170808-90OOooNA-HD-3__R3F8_RN1RP3   \n...                                      ...   \n11995    20171128-22WIooVY-VV-3__R7F4_RN1RP1   \n11996   20150819-13OOooUD-OO-3__R10F2_RN0RP1   \n11997    20170504-25OOooLI-HD-3__R4F3_RN1RP2   \n11998  20170829-111OOooOS-NE-3__R9F10_RN1RP3   \n11999   20160816-13ARooPA-YC-3__R3F19_RN0RP1   \n\n                               r_sampleName  sibling  \\\n0      20150922-07OOcsUD-OO-3__R7F20_RN3RP1        0   \n1       20160419-63OOknAB-CL-3__R5F7_RN1RP3        0   \n2      20170829-71MLbcHE-DI-3__R1F15_RN1RP1        0   \n3      20170829-71ZJbcHE-DI-3__R2F17_RN1RP1        0   \n4       20170601-46OOooNA-HD-3__R4F5_RN1RP1        0   \n...                                     ...      ...   \n11995  20170404-29OOooBA-CL-3__R4F20_RN1RP2        0   \n11996  20150707-07PCknDA-OO-3__R4F14_RN0RP0        0   \n11997   20140701-18XCsaDA-OO-3__R7F6_RN1RP2        0   \n11998   20150707-07GAcsUD-OO-3__R2F7_RN2RP3        0   \n11999  20170614-71OOooOO-DF-3__R9F14_RN1RP2        0   \n\n                                            c_sampleName  \n0      20171128-71NYsaVH-VG-3__R1F2_RN2RP2_^_20150922...  \n1      20161105-13OOicXZ-JW-3__R3F2_RN2RP1_^_20160419...  \n2      20170228-22OOooNA-HD-3__R2F3_RN1RP4_^_20170829...  \n3      20150819-07OObwUD-OO-3__R5F18_RN2RP2_^_2017082...  \n4      20170808-90OOooNA-HD-3__R3F8_RN1RP3_^_20170601...  \n...                                                  ...  \n11995  20171128-22WIooVY-VV-3__R7F4_RN1RP1_^_20170404...  \n11996  20150819-13OOooUD-OO-3__R10F2_RN0RP1_^_2015070...  \n11997  20170504-25OOooLI-HD-3__R4F3_RN1RP2_^_20140701...  \n11998  20170829-111OOooOS-NE-3__R9F10_RN1RP3_^_201507...  \n11999  20160816-13ARooPA-YC-3__R3F19_RN0RP1_^_2017061...  \n\n[12000 rows x 4 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>l_sampleName</th>\n      <th>r_sampleName</th>\n      <th>sibling</th>\n      <th>c_sampleName</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>20171128-71NYsaVH-VG-3__R1F2_RN2RP2</td>\n      <td>20150922-07OOcsUD-OO-3__R7F20_RN3RP1</td>\n      <td>0</td>\n      <td>20171128-71NYsaVH-VG-3__R1F2_RN2RP2_^_20150922...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>20161105-13OOicXZ-JW-3__R3F2_RN2RP1</td>\n      <td>20160419-63OOknAB-CL-3__R5F7_RN1RP3</td>\n      <td>0</td>\n      <td>20161105-13OOicXZ-JW-3__R3F2_RN2RP1_^_20160419...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>20170228-22OOooNA-HD-3__R2F3_RN1RP4</td>\n      <td>20170829-71MLbcHE-DI-3__R1F15_RN1RP1</td>\n      <td>0</td>\n      <td>20170228-22OOooNA-HD-3__R2F3_RN1RP4_^_20170829...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>20150819-07OObwUD-OO-3__R5F18_RN2RP2</td>\n      <td>20170829-71ZJbcHE-DI-3__R2F17_RN1RP1</td>\n      <td>0</td>\n      <td>20150819-07OObwUD-OO-3__R5F18_RN2RP2_^_2017082...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>20170808-90OOooNA-HD-3__R3F8_RN1RP3</td>\n      <td>20170601-46OOooNA-HD-3__R4F5_RN1RP1</td>\n      <td>0</td>\n      <td>20170808-90OOooNA-HD-3__R3F8_RN1RP3_^_20170601...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>11995</th>\n      <td>20171128-22WIooVY-VV-3__R7F4_RN1RP1</td>\n      <td>20170404-29OOooBA-CL-3__R4F20_RN1RP2</td>\n      <td>0</td>\n      <td>20171128-22WIooVY-VV-3__R7F4_RN1RP1_^_20170404...</td>\n    </tr>\n    <tr>\n      <th>11996</th>\n      <td>20150819-13OOooUD-OO-3__R10F2_RN0RP1</td>\n      <td>20150707-07PCknDA-OO-3__R4F14_RN0RP0</td>\n      <td>0</td>\n      <td>20150819-13OOooUD-OO-3__R10F2_RN0RP1_^_2015070...</td>\n    </tr>\n    <tr>\n      <th>11997</th>\n      <td>20170504-25OOooLI-HD-3__R4F3_RN1RP2</td>\n      <td>20140701-18XCsaDA-OO-3__R7F6_RN1RP2</td>\n      <td>0</td>\n      <td>20170504-25OOooLI-HD-3__R4F3_RN1RP2_^_20140701...</td>\n    </tr>\n    <tr>\n      <th>11998</th>\n      <td>20170829-111OOooOS-NE-3__R9F10_RN1RP3</td>\n      <td>20150707-07GAcsUD-OO-3__R2F7_RN2RP3</td>\n      <td>0</td>\n      <td>20170829-111OOooOS-NE-3__R9F10_RN1RP3_^_201507...</td>\n    </tr>\n    <tr>\n      <th>11999</th>\n      <td>20160816-13ARooPA-YC-3__R3F19_RN0RP1</td>\n      <td>20170614-71OOooOO-DF-3__R9F14_RN1RP2</td>\n      <td>0</td>\n      <td>20160816-13ARooPA-YC-3__R3F19_RN0RP1_^_2017061...</td>\n    </tr>\n  </tbody>\n</table>\n<p>12000 rows × 4 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 79
    }
   ],
   "source": [
    "non_sibling_df[['l_sampleName', 'r_sampleName', 'sibling', 'c_sampleName']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "                                l_sampleName  \\\n0      20170808-109OOooNA-JB-3__R10F5_RN1RP3   \n1      20170808-109OOooNA-JB-3__R10F5_RN1RP3   \n2      20170808-109OOooNA-JB-3__R10F5_RN1RP3   \n3      20170808-109OOooNA-JB-3__R10F6_RN1RP4   \n4      20170808-109OOooNA-JB-3__R10F6_RN1RP4   \n...                                      ...   \n10583    20170202-71MZdcEB-DF-3__R7F1_RN1RP1   \n10584    20170202-71MZdcEB-DF-3__R7F2_RN1RP2   \n10585    20170202-71MZdcEB-DF-3__R7F2_RN1RP2   \n10586    20170202-71MZdcEB-DF-3__R7F3_RN1RP3   \n10587    20170202-71MZdcEB-DF-3__R7F3_RN1RP3   \n\n                                r_sampleName  sibling  \\\n0      20170808-109OOooNA-JB-3__R10F6_RN1RP4        1   \n1       20170808-109OOooNA-JB-3__R9F1_RN1RP1        1   \n2       20170808-109OOooNA-JB-3__R9F2_RN1RP2        1   \n3      20170808-109OOooNA-JB-3__R10F5_RN1RP3        1   \n4       20170808-109OOooNA-JB-3__R9F1_RN1RP1        1   \n...                                      ...      ...   \n10583    20170202-71MZdcEB-DF-3__R7F3_RN1RP3        1   \n10584    20170202-71MZdcEB-DF-3__R7F1_RN1RP1        1   \n10585    20170202-71MZdcEB-DF-3__R7F3_RN1RP3        1   \n10586    20170202-71MZdcEB-DF-3__R7F1_RN1RP1        1   \n10587    20170202-71MZdcEB-DF-3__R7F2_RN1RP2        1   \n\n                                            c_sampleName  \n0      20170808-109OOooNA-JB-3__R10F5_RN1RP3_^_201708...  \n1      20170808-109OOooNA-JB-3__R10F5_RN1RP3_^_201708...  \n2      20170808-109OOooNA-JB-3__R10F5_RN1RP3_^_201708...  \n3      20170808-109OOooNA-JB-3__R10F6_RN1RP4_^_201708...  \n4      20170808-109OOooNA-JB-3__R10F6_RN1RP4_^_201708...  \n...                                                  ...  \n10583  20170202-71MZdcEB-DF-3__R7F1_RN1RP1_^_20170202...  \n10584  20170202-71MZdcEB-DF-3__R7F2_RN1RP2_^_20170202...  \n10585  20170202-71MZdcEB-DF-3__R7F2_RN1RP2_^_20170202...  \n10586  20170202-71MZdcEB-DF-3__R7F3_RN1RP3_^_20170202...  \n10587  20170202-71MZdcEB-DF-3__R7F3_RN1RP3_^_20170202...  \n\n[10588 rows x 4 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>l_sampleName</th>\n      <th>r_sampleName</th>\n      <th>sibling</th>\n      <th>c_sampleName</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>20170808-109OOooNA-JB-3__R10F5_RN1RP3</td>\n      <td>20170808-109OOooNA-JB-3__R10F6_RN1RP4</td>\n      <td>1</td>\n      <td>20170808-109OOooNA-JB-3__R10F5_RN1RP3_^_201708...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>20170808-109OOooNA-JB-3__R10F5_RN1RP3</td>\n      <td>20170808-109OOooNA-JB-3__R9F1_RN1RP1</td>\n      <td>1</td>\n      <td>20170808-109OOooNA-JB-3__R10F5_RN1RP3_^_201708...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>20170808-109OOooNA-JB-3__R10F5_RN1RP3</td>\n      <td>20170808-109OOooNA-JB-3__R9F2_RN1RP2</td>\n      <td>1</td>\n      <td>20170808-109OOooNA-JB-3__R10F5_RN1RP3_^_201708...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>20170808-109OOooNA-JB-3__R10F6_RN1RP4</td>\n      <td>20170808-109OOooNA-JB-3__R10F5_RN1RP3</td>\n      <td>1</td>\n      <td>20170808-109OOooNA-JB-3__R10F6_RN1RP4_^_201708...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>20170808-109OOooNA-JB-3__R10F6_RN1RP4</td>\n      <td>20170808-109OOooNA-JB-3__R9F1_RN1RP1</td>\n      <td>1</td>\n      <td>20170808-109OOooNA-JB-3__R10F6_RN1RP4_^_201708...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>10583</th>\n      <td>20170202-71MZdcEB-DF-3__R7F1_RN1RP1</td>\n      <td>20170202-71MZdcEB-DF-3__R7F3_RN1RP3</td>\n      <td>1</td>\n      <td>20170202-71MZdcEB-DF-3__R7F1_RN1RP1_^_20170202...</td>\n    </tr>\n    <tr>\n      <th>10584</th>\n      <td>20170202-71MZdcEB-DF-3__R7F2_RN1RP2</td>\n      <td>20170202-71MZdcEB-DF-3__R7F1_RN1RP1</td>\n      <td>1</td>\n      <td>20170202-71MZdcEB-DF-3__R7F2_RN1RP2_^_20170202...</td>\n    </tr>\n    <tr>\n      <th>10585</th>\n      <td>20170202-71MZdcEB-DF-3__R7F2_RN1RP2</td>\n      <td>20170202-71MZdcEB-DF-3__R7F3_RN1RP3</td>\n      <td>1</td>\n      <td>20170202-71MZdcEB-DF-3__R7F2_RN1RP2_^_20170202...</td>\n    </tr>\n    <tr>\n      <th>10586</th>\n      <td>20170202-71MZdcEB-DF-3__R7F3_RN1RP3</td>\n      <td>20170202-71MZdcEB-DF-3__R7F1_RN1RP1</td>\n      <td>1</td>\n      <td>20170202-71MZdcEB-DF-3__R7F3_RN1RP3_^_20170202...</td>\n    </tr>\n    <tr>\n      <th>10587</th>\n      <td>20170202-71MZdcEB-DF-3__R7F3_RN1RP3</td>\n      <td>20170202-71MZdcEB-DF-3__R7F2_RN1RP2</td>\n      <td>1</td>\n      <td>20170202-71MZdcEB-DF-3__R7F3_RN1RP3_^_20170202...</td>\n    </tr>\n  </tbody>\n</table>\n<p>10588 rows × 4 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 81
    }
   ],
   "source": [
    "sibling_df[['l_sampleName', 'r_sampleName', 'sibling', 'c_sampleName']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = pd.concat([non_sibling_df, sibling_df], axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df.to_csv(str('../03_SourceFiles/03_ProcessedFiles/'+data_type+'-pair-wise-df.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "derda1",
   "name": "derda1"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}