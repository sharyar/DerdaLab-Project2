{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create PairWise Training and Test Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Create a common data frame with 8000*2, 1 label, and n rows of samples. Call this **mainDF**\n",
    "2. Use **sourceDF** to do a stratified random sampling of the samples so we have even distribution of labels. \n",
    "3. Append 2 samples to mainDF and add label based on the samples added\n",
    "4. Labels: Not siblings, siblings\n",
    "5. The label is based on filename again where two samples are siblings if: XXXXX-RPn everything except n matches. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "vscode": {
     "end_execution_time": "2020-08-13T19:44:26.263Z",
     "start_execution_time": "2020-08-13T19:44:26.249Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from fileFunctionsModule import importCSVMotifFilesAsDfFromDir\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import time\n",
    "from datetime import date\n",
    "import sys\n",
    "import os.path\n",
    "import re\n",
    "from itertools import permutations\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Import & Early Processing to a Single DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "vscode": {
     "end_execution_time": "2020-08-13T19:43:21.872Z",
     "start_execution_time": "2020-08-13T19:43:18.626Z"
    }
   },
   "outputs": [],
   "source": [
    "# ! Define variables to be used\n",
    "rerun_data_compilation = False\n",
    "data_path = \"\"\n",
    "data_type = \"\"\n",
    "date_current = str(date.today().isoformat())\n",
    "\n",
    "# Ask user what kind of data needs to be processed and if data should be recompiled from source csv files. \n",
    "while (data_type.lower() not in ['trimer', 'tetramer']) :\n",
    "    data_type = input('Process Trimer or Tetramer data type?')\n",
    "    if (input('Would you like to rerun data compilation? Y or N').upper() == 'N'):\n",
    "        rerun_data_compilation = False\n",
    "    else:\n",
    "        rerun_data_compilation = True\n",
    "\n",
    "if data_type == 'trimer':\n",
    "    data_path = \"../03_SourceFiles/01_TrimerSourceFiles\"\n",
    "elif data_type == 'tetramer':\n",
    "    data_path = \"../03_SourceFiles/02_TetramerSourceFiles\"\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": [],
    "vscode": {
     "end_execution_time": "2020-08-13T19:36:33.974Z",
     "start_execution_time": "2020-08-13T19:36:32.418Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File read succsessful. Data frame loaded into memory.\n"
     ]
    }
   ],
   "source": [
    "# Checks if user wants to rerun data compilation. If not, old label file is used from memory\n",
    "\n",
    "if rerun_data_compilation:\n",
    "    # Import Data\n",
    "    # transpose axis of the dataframe:\n",
    "    # Fill na and nan values with 0 instead\n",
    "    data_start = importCSVMotifFilesAsDfFromDir(data_path).transpose().reset_index().fillna(0)\n",
    "\n",
    "    # Rename index column to sampleName in place\n",
    "    data_start.rename(columns={'index': 'sampleName'}, inplace=True)\n",
    "\n",
    "    # Add a new column with a binary identifier for a naive vs selected library. i.e = 1 is naive and 0 is selected\n",
    "    data_start['naiveLibrary'] = data_start.apply(\n",
    "        lambda row: (('oo' in row.sampleName[10:21]) and\n",
    "                    ('RN0' in row.sampleName[-6:] or 'RN1' in row.sampleName[-6:])),\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "    # convert the data in the column (NaiveLibrary) to int type so it is readable\n",
    "    data_start['naiveLibrary'] = data_start['naiveLibrary'].astype(int)\n",
    "\n",
    "    if (input('Would you like to save the resulting DF as a csv for future use or overwrite the previously saved csv? Y or N').upper() == 'Y'):\n",
    "        data_start.to_csv(str('../03_SourceFiles/03_ProcessedFiles/'+data_type+'-combined-labeled.csv'))\n",
    "        print('DF Structure Saved to file.')\n",
    "    else:\n",
    "        pass\n",
    "elif not rerun_data_compilation:\n",
    "    if os.path.isfile(str('../03_SourceFiles/03_ProcessedFiles/'+data_type+'-combined-labeled.csv')):\n",
    "        try:\n",
    "            data_start = pd.read_csv(\n",
    "                str('../03_SourceFiles/03_ProcessedFiles/'+data_type+'-combined-labeled.csv'),\n",
    "                engine='c',\n",
    "                low_memory=False,\n",
    "                index_col=0\n",
    "                )\n",
    "            print('File read succsessful. Data frame loaded into memory.')\n",
    "        except:\n",
    "            print('Something went wrong while trying to read the csv file. Please rerun data compilation or check file manually.')\n",
    "            sys.exit()\n",
    "    else:\n",
    "        print('CSV File does not exist in the source folder (03_SourceFiles/03_ProcessedFiles). Please re-run compilation')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find # of siblings in data set & separate them out into sibling groups. \n",
    "\n",
    "1. Create a naive and a selected set. \n",
    "2. Create two dictionaries: one containing groups of naive siblings and one for selected siblings. \n",
    "3. Enumerate sets to use for stats later. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "vscode": {
     "end_execution_time": "2020-08-13T19:36:38.534Z",
     "start_execution_time": "2020-08-13T19:36:38.457Z"
    }
   },
   "outputs": [],
   "source": [
    "naive_sampleName_set = data_start[(data_start['naiveLibrary'] == 1)]['sampleName']\n",
    "selected_sampleName_set = data_start[~(data_start['naiveLibrary'] == 1)]['sampleName']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logic of setting up groups:\n",
    "\n",
    "1. Set key of dictionary to 20170808-109OOooNA-JB-3_RN**X** where X indicates round. The Primer and Replicate are ommitted\n",
    "2. Append value of sampleName to list within each diction by matching the key. Use Regular Expressions to create and match key. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "vscode": {
     "end_execution_time": "2020-08-13T19:36:44.916Z",
     "start_execution_time": "2020-08-13T19:36:44.901Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define required functions\n",
    "\n",
    "def find_sibling_groups(data_iterable):\n",
    "    \"\"\"\n",
    "    Creates a dictionary from provided iterable. The iterable should be made up of sampleNames in this format: \n",
    "    20170808-109OOooNA-JB-3__R10F5_RN1RP3\n",
    "    The keys of the dictionary are set to: 20170808-109OOooNA-JB-3_RN1 (The primer and replicate info is removed)\n",
    "    \"\"\"\n",
    "    sibling_group_dict = {}\n",
    "\n",
    "    for index, value in data_iterable.items():\n",
    "        keyID = value.split('__')[0] + '_' + re.findall(r'RN\\d{1,}' , value)[0]\n",
    "        if keyID not in sibling_group_dict:\n",
    "            sibling_group_dict[keyID] = [value]\n",
    "        else:\n",
    "            if value not in sibling_group_dict[keyID]:\n",
    "                sibling_group_dict[keyID].append(value)\n",
    "    print('Sibbling Groups Founds: ', len(sibling_group_dict))\n",
    "    return sibling_group_dict\n",
    "\n",
    "\n",
    "def remove_single_member_sibling_groups(sibling_group_original, return_extra=False):\n",
    "    \"\"\"\n",
    "    Iterates through the provided dictionary and finds any sibblings groups with one member and removes them.\n",
    "    Returns three values: \n",
    "        sibbling group as a dict with single groups removed\n",
    "        single_group_value -> values within the single groups\n",
    "        empty_group -> returns any keys in dictionary which had empty lists (unlikely to happen)\n",
    "\n",
    "    \"\"\"\n",
    "    sibling_group = sibling_group_original.copy()\n",
    "    single_group_key = []\n",
    "    single_group_value = []\n",
    "    empty_group = []\n",
    "    for key, value in sibling_group.items():\n",
    "        if len(value) == 1:\n",
    "            single_group_value.append(value)\n",
    "            single_group_key.append(key)\n",
    "        elif len(value) < 1:\n",
    "            empty_group.append(key)\n",
    "    \n",
    "    for key_ in single_group_key:\n",
    "        sibling_group.pop(key_)\n",
    "    for key_ in empty_group:\n",
    "        sibling_group.pop(key_)\n",
    "    if len(single_group_key) > 0:\n",
    "        print('Single Groups Found: ', len(single_group_value), '\\nSibling Groups: ', len(sibling_group))\n",
    "    if len(empty_group) > 0:\n",
    "        print(\"Empty Groups Found\")\n",
    "\n",
    "    if return_extra:\n",
    "        return sibling_group, single_group_value, empty_group\n",
    "    else:\n",
    "        return sibling_group\n",
    "\n",
    "def remove_mismatching_sibling_groups(sibling_group_original, return_extra=False):\n",
    "    \"\"\"\n",
    "    Iterates through provided sibbling dictionary to find any mismatches beteween key name and values within the list associated with the key. \n",
    "    Return_extra is set to false by default so only the sibbling_group dictionary is return. Can be changed to return:\n",
    "        sibbling_group -> Dictionary\n",
    "        mismatching_groups -> Dictionary (includes the entire group that had a mismatch)\n",
    "        mismatching_groups -> Values that were mismatched (without their keys)\n",
    "    \"\"\"\n",
    "\n",
    "    sibling_group = sibling_group_original.copy()\n",
    "    mismatching_groups ={}\n",
    "    mismatching_group_values = []\n",
    "    for key, value in sibling_group.items():\n",
    "        group_id = re.findall(r'RN\\d{1,}', key)[0]\n",
    "        for sample in value:\n",
    "            if group_id != re.findall(r'RN\\d{1,}', sample)[0]:\n",
    "                mismatching_groups[key] = value\n",
    "                mismatching_group_values.append(sample)\n",
    "\n",
    "    for key in mismatching_groups:\n",
    "        sibling_group.pop(key)\n",
    "\n",
    "    if len(mismatching_groups) > 0:\n",
    "        print('Mismatches Found, rerun with return_extra=True as an argument to get mismatching values')\n",
    "    \n",
    "    if (return_extra):\n",
    "        return sibling_group, mismatching_groups, mismatching_group_values\n",
    "    else:\n",
    "        return sibling_group\n",
    "\n",
    "def find_average_sibling_members(sibling_group):\n",
    "    total_members = 0\n",
    "    for key, value in sibling_group.items():\n",
    "        total_members += len(value)\n",
    "    return total_members/(len(sibling_group))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sibbling Groups Founds:  255\n",
      "Single Groups Found:  53 \n",
      "Sibling Groups:  202\n",
      "Sibbling Groups Founds:  448\n",
      "Single Groups Found:  73 \n",
      "Sibling Groups:  375\n"
     ]
    }
   ],
   "source": [
    "#### Create Sibbling Groups:\n",
    "naive_sibling_group_dict = find_sibling_groups(naive_sampleName_set)\n",
    "naive_sibling_group_dict = remove_mismatching_sibling_groups(remove_single_member_sibling_groups(naive_sibling_group_dict))\n",
    "\n",
    "selected_sibling_group_dict = find_sibling_groups(selected_sampleName_set)\n",
    "selected_sibling_group_dict = remove_mismatching_sibling_groups(remove_single_member_sibling_groups(selected_sibling_group_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Combine the two sibling dictionaries into one:\n",
    "\n",
    "combined_sibling_group_dict = {**naive_sibling_group_dict, **selected_sibling_group_dict}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split_sibling_dict(sibling_dict, split_ratio=0.1, random_seed=42):\n",
    "    \"\"\"\n",
    "    Returns a train andd test dictionary of siblings based on the provided split ratio. \n",
    "\n",
    "        Parameters:\n",
    "            sibling_dict (dict): a dictionary composed of siblings with the key being the sibling identifier and the value being a list of siblings for that identifier\n",
    "            split_ratio (float): the ratio of training to test siblings. Default value = 0.1\n",
    "            random_seed (int): the seed to set random to. By default 42. \n",
    "\n",
    "        Returns:\n",
    "            train_split (dict): A dictionary containing the siblings to use for training. \n",
    "            test_split (dict): A dictionary containing the sibling to usee for testing. \n",
    "\n",
    "    \"\"\"\n",
    "    random.seed = random_seed\n",
    "    train_split = dict(random.sample(sibling_dict.items(), int(len(sibling_dict.keys())*(1.0-split_ratio))))\n",
    "    test_split = {k: v for (k, v) in sibling_dict.items() if k not in train_split}\n",
    "\n",
    "    return train_split, test_split;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate test and train split of sibling dictionary\n",
    "\n",
    "The test set will be used only to generate the non-siblings and siblings for the test df. BE CAREFUL OF THIS!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dict, test_dict = train_test_split_sibling_dict(combined_sibling_group_dict, 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extra for stats only\n",
    "# labels_bar_siblings = ['Naive SG', 'Selected SG', 'Naive Singles', 'Selected Singles']\n",
    "# data_bar_siblings = [\n",
    "#     len(naive_sibling_groups), \n",
    "#     len(selected_sibling_groups),\n",
    "#     len(n_s_g),\n",
    "#     len(s_s_g)\n",
    "#     ]\n",
    "# plt.bar(labels_bar_siblings, data_bar_siblings)\n",
    "# plt.title('Number of Sibling Groups')\n",
    "# plt.xlabel('Singles Indicate Sibling Groups with 1 Member',fontdict={\n",
    "#     'weight':'normal',\n",
    "#     'size':'13'\n",
    "# },  labelpad=10)\n",
    "\n",
    "# plt.savefig(str(data_type+'-sibling-groups-distribution-'+ date_current + '.png'), bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.7475247524752477\n",
      "4.037333333333334\n"
     ]
    }
   ],
   "source": [
    "print(find_average_sibling_members(naive_sibling_group_dict))\n",
    "print(find_average_sibling_members(selected_sibling_group_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_sibling_list(sibling_dict):\n",
    "    list_left = []\n",
    "    list_right = []\n",
    "    for key,value in sibling_dict.items():\n",
    "        for a,b in permutations(value, 2):\n",
    "            list_left.append(a)\n",
    "            list_right.append(b)\n",
    "        \n",
    "    return list_left, list_right\n",
    "\n",
    "def generate_df_from_permutations(df, sibling_dict, sibling_label):\n",
    "    if sibling_label:\n",
    "        list_left, list_right = create_sibling_list(sibling_dict)\n",
    "    else:\n",
    "        list_left, list_right = zip(*sibling_dict)\n",
    "    df_left = pd.DataFrame(list_left, columns=['sampleName'], index=None)\n",
    "    df_left = pd.merge(df_left, df, how='left', on='sampleName', sort=False, validate='m:1')\n",
    "    df_right = pd.DataFrame(list_right, columns=['sampleName'], index=None)\n",
    "    df_right = pd.merge(df_right, df, how='left', on='sampleName', sort=False, validate='m:1')\n",
    "    df_left = df_left.add_prefix('l_')\n",
    "    df_right = df_right.add_prefix('r_')\n",
    "    df_combined = pd.concat([df_left, df_right], axis=1)\n",
    "    df_combined['sibling'] = sibling_label\n",
    "    df_combined['c_sampleName'] = df_combined['l_sampleName'] + '_^_' + df_combined['r_sampleName']\n",
    "    print('DF Created Successfully. Df shape: \\n', df_combined.shape)\n",
    "    return df_combined\n",
    "\n",
    "def generate_random_non_sibling_df(df, sibling_dict, sample_size=10000, r_seed=42):\n",
    "    # converts the lists of lists within the sibling_dict to a flat list\n",
    "    flat_list = set([item for sublist in sibling_dict.values() for item in sublist])\n",
    "    all_permutations = set(permutations(flat_list, 2))\n",
    "    print('All Possible Permutations:', len(list(all_permutations)))\n",
    "    \n",
    "    list_left, list_right = create_sibling_list(sibling_dict)\n",
    "    permutations_siblings = set(tuple(zip(list_left, list_right)))\n",
    "    print('Sibling Permutations: ', len(permutations_siblings))\n",
    "\n",
    "    permutation_non_sibling_dict = all_permutations.difference(permutations_siblings)\n",
    "    print('Non-Sibling Permutations: ', len(permutation_non_sibling_dict))\n",
    "\n",
    "    random.seed(r_seed)\n",
    "    print('Sampling Seed Used: ', r_seed)\n",
    "    permutation_non_sibling_sampled_dict = random.sample(permutation_non_sibling_dict, sample_size)\n",
    "\n",
    "    return generate_df_from_permutations(df, permutation_non_sibling_sampled_dict, 0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DF Created Successfully. Df shape: \n",
      " (8268, 16006)\n"
     ]
    }
   ],
   "source": [
    "sibling_train_df = generate_df_from_permutations(data_start, train_dict, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DF Created Successfully. Df shape: \n",
      " (2320, 16006)\n"
     ]
    }
   ],
   "source": [
    "sibling_test_df = generate_df_from_permutations(data_start, test_dict, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All Possible Permutations: 3205890\n",
      "Sibling Permutations:  8268\n",
      "Non-Sibling Permutations:  3197622\n",
      "Sampling Seed Used:  42\n",
      "DF Created Successfully. Df shape: \n",
      " (9000, 16006)\n"
     ]
    }
   ],
   "source": [
    "non_sibling_train_df = generate_random_non_sibling_df(data_start, train_dict, 9000, 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All Possible Permutations: 229920\n",
      "Sibling Permutations:  2320\n",
      "Non-Sibling Permutations:  227600\n",
      "Sampling Seed Used:  42\n",
      "DF Created Successfully. Df shape: \n",
      " (3000, 16006)\n"
     ]
    }
   ],
   "source": [
    "non_sibling_test_df = generate_random_non_sibling_df(data_start, test_dict, 3000, 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "(3000, 16006)"
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "non_sibling_test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.concat([non_sibling_train_df, sibling_train_df], axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.concat([non_sibling_test_df, sibling_test_df], axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "(17268, 16006)"
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "(5320, 16006)"
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.to_csv(str('../03_SourceFiles/03_ProcessedFiles/'+data_type+'-pair-wise-train-df.csv'))\n",
    "test_df.to_csv(str('../03_SourceFiles/03_ProcessedFiles/'+data_type+'-pair-wise-test-df.csv'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('derda1': conda)",
   "name": "python_defaultSpec_1597347368408"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}